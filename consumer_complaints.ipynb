{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d15c970",
   "metadata": {
    "id": "0d15c970"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "plt.style.use('ggplot')\n",
    "np.set_printoptions(precision=3)\n",
    "pd.set_option('precision', 3)\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import optuna\n",
    "import lightgbm\n",
    "import xgboost\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder, PowerTransformer, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.feature_selection import SelectPercentile, SelectKBest, mutual_info_classif, mutual_info_regression\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, train_test_split\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "import imblearn\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn import over_sampling\n",
    "from imblearn import combine\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ESXgWQCK68MX",
   "metadata": {
    "id": "ESXgWQCK68MX"
   },
   "outputs": [],
   "source": [
    "def get_top_k_frequent_percentage(col, k=10):\n",
    "    return df[col].value_counts().head(k).sum()*100/df.shape[0]\n",
    "def get_top_k_frequent_items(col, k=10):\n",
    "    return set(df[col].value_counts().head(k).index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4117c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineered_data_dir_path = 'feature_engineered_data/'\n",
    "feature_engineered_data_dir = os.listdir(feature_engineered_data_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67426ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if len(feature_engineered_data_dir)\n",
    "\n",
    "    print('x_train, x_test, y_train, y_test file creation start ............')\n",
    "    consumer_complaints = pd.read_csv('consumer_complaints.csv')\n",
    "    df = consumer_complaints.copy()\n",
    "    rename_cols = {'Date received': 'date_received', 'Product':'product', 'Sub-product':'sub_product', \n",
    "                   'Sub-issue':'sub_issue', 'Consumer complaint narrative': 'complaint', 'Issue':'issue',\n",
    "                   'Company public response': 'response_to_public', 'Company':'company', 'State':'state',\n",
    "                  'ZIP code':'zip', 'Tags':'tags', 'Consumer consent provided?': 'consent_provided',\n",
    "                  'Submitted via': 'submitted_via', 'Date sent to company':'date_sent',\n",
    "                  'Company response to consumer':'response_to_consumer', 'Timely response?':'timely_response',\n",
    "                  'Consumer disputed?':'consumer_disputed', 'Complaint ID':'complaint_id'}\n",
    "    df.rename(columns=rename_cols, inplace=True)\n",
    "\n",
    "    df['date_received'] = pd.to_datetime(df['date_received'], format=\"%m/%d/%Y\")\n",
    "    df['date_sent'] = pd.to_datetime(df['date_sent'], format=\"%m/%d/%Y\")\n",
    "\n",
    "    df.drop('complaint_id', axis=1, inplace=True)\n",
    "    target = 'response_to_consumer'\n",
    "\n",
    "    \"\"\" Fill Null Values \"\"\"\n",
    "    print('filling null values ................')\n",
    "    s = df.isna().sum()*100/df.shape[0] > 0\n",
    "    high_null_col = s[s].index.tolist()\n",
    "\n",
    "    df['complaint'] = 1 - df['complaint'].isna().astype(int)\n",
    "    df['response_to_public'], index_response_to_public = pd.factorize(df['response_to_public'])\n",
    "    df['tags'], index_tags = pd.factorize(df['tags'])\n",
    "    df['consent_provided'], index_consent_provided = pd.factorize(df['consent_provided'])\n",
    "\n",
    "    # df.groupby('sub_issue')[target].value_counts().to_frame().head(300).head(50)\n",
    "    ### checked to make sure that there is no particular relevance of 'sub_issue' to 'response_to_consumer'\n",
    "\n",
    "    sub_issue_top = df['sub_issue'].value_counts().head(6).index.tolist()\n",
    "    df['sub_issue'] = df['sub_issue'].apply(lambda x: 'others' if (not x is np.nan) and (x not in sub_issue_top) else x)\n",
    "    df['sub_issue'], index_sub_issue = pd.factorize(df['sub_issue'])\n",
    "\n",
    "    cols = ['state', 'zip', 'consumer_disputed']\n",
    "    df.loc[:, cols] = df[cols].fillna(method='ffill') # used ffill since % na is less and data is a bit sequential\n",
    "\n",
    "    col = 'sub_product'\n",
    "    df_sub_prod_fill = df[~df['sub_product'].isna()].groupby(['product'])[col].apply(lambda x: x.mode().iloc[0]).to_frame().reset_index()\n",
    "\n",
    "    df['sub_product'].fillna(value='credit', inplace=True)\n",
    "\n",
    "    \"\"\" Categorical Encoding \"\"\"\n",
    "    print('categorical encoding is on progress ..............')\n",
    "    yes_no_dct = {'Yes':1, 'No':0}\n",
    "    df['timely_response'] = df['timely_response'].apply(lambda x: yes_no_dct[x])\n",
    "    df['consumer_disputed'] = df['consumer_disputed'].apply(lambda x: yes_no_dct[x])\n",
    "\n",
    "    df['time_delta'] = (df['date_sent'] - df['date_received']).dt.days\n",
    "\n",
    "    for col in ['date_received', 'date_sent']:\n",
    "        df[col+'_day'] = df[col].dt.day\n",
    "        df[col+'_month'] = df[col].dt.month\n",
    "        df[col+'_quarter'] = df[col].dt.quarter\n",
    "        df[col+'_year'] = df[col].dt.year\n",
    "\n",
    "    \"\"\"After rounding `zip` code to last 3 places, and filling null values using `groupby` and `median`,\n",
    "    we check unique values of `zip` grouping by `state` and find that most of the zip codes are concentrated\n",
    "    on 1-3 zip codes within a state, which is not accounting for much variance given `state` info.\n",
    "    Even if we pick up top few `zip` codes and replace the rest by the median values using groupby on state,\n",
    "    it will not add any info besides what we have from `state` info, but will create many more columns after\n",
    "    one hot encoding. Hence, we will drop `zip` feature.\"\"\"\n",
    "\n",
    "    cols_to_drop = ['date_received', 'date_sent', 'zip']\n",
    "    cat_cols = df.select_dtypes(['object', 'datetime64']).columns.tolist()\n",
    "    cat_cols = list(set(cat_cols).difference(set(cols_to_drop)))\n",
    "\n",
    "    freq_company = get_top_k_frequent_items('company', 12)\n",
    "    df['company'] = df['company'].apply(lambda x: x if x in freq_company else 'Others')\n",
    "\n",
    "    freq_issue = get_top_k_frequent_items('issue', 8)\n",
    "    df['issue'] = df['issue'].apply(lambda x: x if x in freq_issue else 'others')\n",
    "\n",
    "    df['response'] = df['response_to_consumer']\n",
    "    le = LabelEncoder()\n",
    "    df['response_to_consumer'] = le.fit_transform(df['response_to_consumer'])\n",
    "\n",
    "    encoded_labels = le.inverse_transform([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "    cols_to_drop = ['date_received', 'date_sent', 'zip', 'sub_product', 'response'] # , 'issue' , 'company', 'state']\n",
    "    dfc = df.copy()\n",
    "    dfc.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    dfc = pd.get_dummies(dfc)\n",
    "    X = dfc.drop(columns='response_to_consumer')\n",
    "    y = dfc['response_to_consumer']\n",
    "\n",
    "    \"\"\" Divide dataset into train, test \"\"\"\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "    # if not os.path.exists('x_train.csv'):\n",
    "    df_train = pd.concat([x_train, y_train], axis=1)\n",
    "    df_target_mean = df_train.groupby(cols)[target].apply(lambda x: x.mean()).to_frame().reset_index()\n",
    "    df_target_mean.rename(columns={target : target+'_mean'}, inplace=True)\n",
    "    df_target_count_percent = df_train.groupby(cols)[target].apply(lambda x: len(x)*100/df_train.shape[0]).to_frame().reset_index()\n",
    "    df_target_count_percent.rename(columns={target : target+'_count_percent'}, inplace=True)\n",
    "\n",
    "    x_train = pd.merge(x_train, df_target_mean, how='left', left_on=cols, right_on=cols)\n",
    "    x_test = pd.merge(x_test, df_target_mean, how='left', left_on=cols, right_on=cols)\n",
    "    x_train = pd.merge(x_train, df_target_count_percent, how='left', left_on=cols, right_on=cols)\n",
    "    x_test = pd.merge(x_test, df_target_count_percent, how='left', left_on=cols, right_on=cols)\n",
    "\n",
    "    # if not os.path.exists('x_train.csv'):\n",
    "    x_train.to_csv('x_train.csv')\n",
    "    y_train.to_csv('y_train.csv')\n",
    "    x_test.to_csv('x_test.csv')\n",
    "    y_test.to_csv('y_test.csv')\n",
    "\n",
    "else:\n",
    "    x_train = pd.read_csv('x_train.csv', index_col=0)\n",
    "    y_train = pd.read_csv('y_train.csv', index_col=0)\n",
    "    x_test = pd.read_csv('x_test.csv', index_col=0)\n",
    "    y_test = pd.read_csv('y_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D78GNRIdlb3d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D78GNRIdlb3d",
    "outputId": "7a972026-8788-42ae-e0d0-80ddb4eeef56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time-taken:  1206.8394 s\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Target Encoding feature engineering \"\"\"\n",
    "start_time = time.time()\n",
    "mi_scores = mutual_info_classif(x_train, y_train)\n",
    "print(f'time-taken to compute mi_scores: {time.time()-start_time : 0.4f} s')\n",
    "\n",
    "mi_scores = pd.Series(data=mi_scores, index=x_train.columns.to_list())\n",
    "mi_scores.sort_values(ascending=False, inplace=True)\n",
    "k = 10\n",
    "bar_plot = sns.barplot(x=mi_scores.head(k).values, y=mi_scores.head(k).index)\n",
    "bar_plot.get_figure().savefig('mi_scores.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c73029",
   "metadata": {
    "id": "98c73029"
   },
   "source": [
    "## Other Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c42493cd",
   "metadata": {
    "id": "c42493cd"
   },
   "outputs": [],
   "source": [
    "def get_ann_model():\n",
    "    model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(120,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(8, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def get_value_counts(y):\n",
    "    y_classes, y_counts = np.unique(y, return_counts=True)\n",
    "    return {key:val for (key, val) in zip(y_classes, y_counts)}\n",
    "\n",
    "def get_sns_heatmap(conf_mat, name, balance_algo, f1, encoded_labels=encoded_labels):\n",
    "    heat_dir = consumer_complaints_path+'heat_plots/'\n",
    "    if not os.path.isdir(heat_dir):\n",
    "        os.mkdir(heat_dir)\n",
    "    conf_mat = pd.DataFrame(conf_mat, index=encoded_labels, columns=encoded_labels)\n",
    "    heat_plot = sns.heatmap(conf_mat, annot=True, cmap='Blues', fmt='g')\n",
    "    fig = heat_plot.get_figure()\n",
    "    fig.suptitle(f'f1-macro: {f1: 0.3f}')\n",
    "    fig_path = heat_dir+name+'_'+balance_algo+'.png'\n",
    "    fig.savefig(fig_path)\n",
    "    print(f'{fig_path} saved')\n",
    "    return heat_plot\n",
    "\n",
    "def model_name(model):\n",
    "    return model.__class__.__name__\n",
    "\n",
    "def get_balance_algo(get_func):\n",
    "    if get_func == get_f1_score:\n",
    "        return 'baseline'\n",
    "    elif get_func == get_f1_confusion_mat_SMOTE:\n",
    "        return 'SMOTE'\n",
    "    elif get_func == get_f1_confusion_mat_custom_SMOTETomek:\n",
    "        return 'custom_SMOTETomek'\n",
    "    elif get_func == get_f1_confusion_mat_hyperparameter_class_wt:\n",
    "        return 'class_weight'\n",
    "    else:\n",
    "        raise KeyError(f'{get_func} does not exist')\n",
    "\n",
    "def model_f1_conf_mat(model, x_test=x_test, y_test=y_test):\n",
    "    if model_name(model) != 'Sequential':\n",
    "        y_test_pred = model.predict(x_test)\n",
    "    else:\n",
    "        y_test_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "    f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "    conf_mat = confusion_matrix(y_test, y_test_pred)\n",
    "    return f1, conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aaeb5d",
   "metadata": {
    "id": "84aaeb5d"
   },
   "source": [
    "## create df_result to save results     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "318d7db1",
   "metadata": {
    "id": "318d7db1"
   },
   "outputs": [],
   "source": [
    "models = [lightgbm.LGBMClassifier(), xgboost.XGBClassifier(eval_metric='mlogloss'), SVC(), get_ann_model(), KNeighborsClassifier()]\n",
    "index = [model_name(model) for model in models]\n",
    "columns = ['baseline', 'SMOTE', 'custom_SMOTETomek', 'class_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab19fd46",
   "metadata": {
    "id": "ab19fd46"
   },
   "outputs": [],
   "source": [
    "file_result_path = 'df_result.csv'\n",
    "if not os.path.exists(file_result_path):\n",
    "    nrows = len(index)\n",
    "    ncols = len(columns)\n",
    "    data = [['NA']*ncols]*nrows\n",
    "    df_result = pd.DataFrame(data=data, index=index, columns=columns)\n",
    "    df_result.to_csv(file_result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bGSVgBqxTyuw",
   "metadata": {
    "id": "bGSVgBqxTyuw"
   },
   "outputs": [],
   "source": [
    "file_train_time_path = 'df_train_time.csv'\n",
    "if not os.path.exists(file_train_time):\n",
    "    nrows = len(index)\n",
    "    ncols = len(columns)\n",
    "    data = [['NA']*ncols]*nrows\n",
    "    df_time = pd.DataFrame(data=data, index=index, columns=columns)\n",
    "    df_time.to_csv(file_train_time_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7YkXBcD-VyNi",
   "metadata": {
    "id": "7YkXBcD-VyNi"
   },
   "outputs": [],
   "source": [
    "def get_write_result(model, get_func):\n",
    "    name = model_name(model)\n",
    "    balance_algo = get_balance_algo(get_func)\n",
    "    start_time = time.time()\n",
    "    print(name)\n",
    "    f1, conf_mat = get_func(model)\n",
    "    train_time = (time.time()-start_time)/60\n",
    "    print(f'train-time: {train_time : 0.2f} min')\n",
    "    print('f1_macro:', f1)\n",
    "    get_sns_heatmap(conf_mat)\n",
    "    df_result.loc[name, balance_algo] = f1\n",
    "    df_result.to_csv(file_result)\n",
    "    df_time.loc[name, balance_algo] = train_time\n",
    "    df_time.to_csv(file_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c79da71",
   "metadata": {
    "id": "3c79da71"
   },
   "outputs": [],
   "source": [
    "\"\"\" 1. F1 score without using over-sampling SMOTE (baseline) \"\"\"\n",
    "def get_f1_score(model = lightgbm.LGBMClassifier(), x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test):\n",
    "    if model_name(model) != 'Sequential':\n",
    "        model.fit(x_train, y_train)\n",
    "    else:\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(min_delta=1e-5, patience=5)\n",
    "        model.fit(x_train, y_train, validation_split=0.3, epochs=80, \n",
    "                  callbacks=[early_stopping, TqdmCallback(verbose=0)], verbose=0)\n",
    "    return model_f1_conf_mat(model)\n",
    "\n",
    "for model in models:\n",
    "    get_write_result(model, get_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4d3bd7",
   "metadata": {
    "id": "ce4d3bd7"
   },
   "outputs": [],
   "source": [
    "\"\"\" 2. F1 score using over-sampling SMOTE \"\"\"\n",
    "def get_f1_confusion_mat_SMOTE(model, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test):\n",
    "    smt = over_sampling.SMOTE(random_state=7)\n",
    "    X_smt, y_smt = smt.fit_resample(x_train, y_train)\n",
    "    if model_name(model) != 'Sequential':\n",
    "        model.fit(X_smt, y_smt)\n",
    "    else:\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(min_delta=1e-5, patience=5)\n",
    "        model.fit(X_smt, y_smt, validation_split=0.3, epochs=80, \n",
    "                  callbacks=[early_stopping, TqdmCallback(verbose=0)], verbose=0)\n",
    "    return model_f1_conf_mat(model)\n",
    "\n",
    "for model in models:\n",
    "    get_write_result(model, get_f1_confusion_mat_SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73796218",
   "metadata": {
    "id": "73796218"
   },
   "outputs": [],
   "source": [
    "\"\"\" 3. F1 score using over-sampling SMOTETomek \"\"\"\n",
    "def get_f1_confusion_mat_custom_SMOTETomek(model, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test):\n",
    "    sampling_strategy = {0:100000, 1:395185, 2:100000, 3:100000, 4:50000, 5:100000, 6:50000, 7:50000}\n",
    "    sm = combine.SMOTETomek(sampling_strategy=sampling_strategy, random_state=7)\n",
    "    X_sm, y_sm = sm.fit_resample(x_train, y_train)\n",
    "    if model_name(model) != 'Sequential':\n",
    "        model.fit(X_sm, y_sm)\n",
    "    else:\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(min_delta=1e-5, patience=5)\n",
    "        model.fit(X_sm, y_sm, validation_split=0.3, epochs=80, \n",
    "                  callbacks=[early_stopping, TqdmCallback(verbose=0)], verbose=0)\n",
    "    return model_f1_conf_mat(model)\n",
    "\n",
    "for model in models:\n",
    "    get_write_result(model, get_f1_confusion_mat_custom_SMOTETomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bffb09e",
   "metadata": {
    "id": "3bffb09e"
   },
   "outputs": [],
   "source": [
    "\"\"\" 4. F1 score using hyperparameter tuning inside algorithm to handle imbalance \"\"\"\n",
    "def get_f1_confusion_mat_hyperparameter_class_wt(model, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test):\n",
    "    cls_wts = class_weight.compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "    if model_name(model) != 'Sequential':\n",
    "        model.fit(x_train, y_train, sample_weight=cls_wts)\n",
    "    else:\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(min_delta=1e-5, patience=5)\n",
    "        model.fit(x_train, y_train, sample_weight=cls_wts, validation_split=0.3, epochs=80, callbacks=[early_stopping, TqdmCallback(verbose=0)], verbose=0)\n",
    "    return model_f1_conf_mat(model)\n",
    "\n",
    "for model in models:\n",
    "    get_write_result(model, get_f1_confusion_mat_hyperparameter_class_wt)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "consumer_complaints_v2_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
